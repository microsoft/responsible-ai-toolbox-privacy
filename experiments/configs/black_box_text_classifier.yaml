defaults:
  - workspace: M365Research-PPML-EUS
  - _self_

shared_training_parameters:
  target_epsilon: 8.0
  num_train_epochs: 3.0
  learning_rate: 5e-4
  per_device_train_batch_size: 32
  gradient_accumulation_steps: 32
  per_sample_max_grad_norm: 0.1
  max_sequence_length: 67
  delta: 1e-5
  model_name: roberta-base

shared_inference_parameters:
  batch_size: 256

game_config:
  num_models: 1024
  seed: 1920
  num_concurrent_jobs_per_node: 8

shadow_model_config:
  num_models: 512
